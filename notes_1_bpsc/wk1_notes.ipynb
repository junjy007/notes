{
 "metadata": {
  "name": "",
  "signature": "sha256:efa813b8b2f0056e9ff37c7858fc25b2ee16cb5009929c8b5194db3305994a63"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy\n",
      "from sklearn import datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Back propagation\n",
      "Now let's make a neural net. Each unit $i$ has *total* input $x_i$, and output $y_i$. The nonlinear activation is $a_i=\\frac{1}{1+e^{-x_i}}$.\n",
      "\n",
      "The activation can be understood as follows:\n",
      "$$\n",
      "y = \\frac{e^x}{1+e^x}\n",
      "$$\n",
      "So it is $e^x$ competing with $e^0==1$. When $x=0$, they draw, then the output is $1/2$. When $x > 0$, $e^x$ takes most portion of the sum, and the quotient becomes 1. When $x< 0$, 1 takes most of the sum. The quotient becomes 0. So the activation is a \"hidden\" softmax. A unit is competing with its dual (which saying the status should be 0) to be activate."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The derivative w.r.t. $x$: \n",
      "$$\n",
      "\\frac{d}{dx} e^x\\times (1+e^x)^{-1} = \n",
      "= e^x (1+e^x)^{-1} + e^x \\times (-1) \\times (1+e^x)^{-2} e^x\n",
      "$$\n",
      "The first part is $y$ itself (thanks the factor $e^x$), the second part is $-y^2$. (Maybe you have a better way of explaining it.) "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each layer represents a number of neurons, say, $q$, they all accepts the same inputs, say, $p$ of them, and generate one output. \n",
      "\n",
      "Each function accepts data of $n$ samples, each sample consisting of the $p$ inputs of the neurons. So the layer process $n \\times p$ data matrix into $n \\times q$ output.\n",
      "\n",
      "In back-propagation, the layer gets information of how the output of each of its neurons affects the final error of each sample, as a $n \\times q$ matrix. It attributes the influence back to its inputs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's take some neurons to make a layer of a network.\n",
      "class NLayer:    \n",
      "    def __init__(self, n_in, n_neurons, act=None, dact=None):\n",
      "        rng = np.random.RandomState(0)\n",
      "        wr = 1/n_in\n",
      "        self.W = rng.uniform(-wr, +wr, (n_in, n_neurons))\n",
      "        self.b = rng.uniform(-.5, +.5, (n_neurons,))\n",
      "        if act is None:\n",
      "            #self.act = scipy.special.expit\n",
      "            self.act = lambda x: 1/(1+np.exp(x))\n",
      "            self.dact = lambda x: self.act(x)*(1-self.act(x))\n",
      "        else:\n",
      "            self.act = act\n",
      "            self.dact = dact\n",
      "    \n",
      "    def forward(self, inputs):\n",
      "        # inputs: [samples x p-input variables]\n",
      "        self.inputs = inputs\n",
      "        self.tot_inputs = np.dot(inputs, self.W) + self.b[np.newaxis,:]\n",
      "        self.outputs = self.act(self.tot_inputs)\n",
      "        self.d_outputs_tot_inputs = self.dact(self.outputs)\n",
      "        \n",
      "    def backprop(self, de):\n",
      "        # de: how the output of each neuron influences the error\n",
      "        n = de.shape[0]\n",
      "        self.delta = de\n",
      "        self.de_tot_input = self.d_outputs_tot_inputs*de\n",
      "        self.de_inputs = np.dot(self.de_tot_input, self.W.T)\n",
      "        self.dW = np.dot(self.inputs.T, self.de_tot_input) / n\n",
      "        self.db = np.sum(self.de_tot_input, axis=0) / n\n",
      "        \n",
      "    def apply_grad(self, learning_rate):\n",
      "        self.W -= self.dW*learning_rate\n",
      "        self.b -= self.db*learning_rate\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Fine! Now let's build two-layer network to predict house prices!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hpdat = datasets.load_boston()\n",
      "from sklearn.cross_validation import train_test_split\n",
      "trn_x, tst_x, trn_y, tst_y = train_test_split(hpdat.data, hpdat.target[:,np.newaxis], test_size=0.2)\n",
      "trn_x, vld_x, trn_y, vld_y = train_test_split(trn_x, trn_y, test_size=0.2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = np.arange(0, np.pi, 0.05)\n",
      "y1 = np.cos(t)\n",
      "y2 = np.sin(t)\n",
      "trn_x = t[:,np.newaxis]\n",
      "trn_y = np.asarray([y1, y2]).T\n",
      "print trn_x.shape\n",
      "print trn_y.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_hidden = 20\n",
      "nn0 = NLayer(n_in=trn_x.shape[1], n_neurons=n_hidden)\n",
      "lin_act = lambda x:x\n",
      "lin_dact = lambda x:np.ones_like(x)\n",
      "nn1 = NLayer(n_in=n_hidden, n_neurons=trn_y.shape[1], act=lin_act, dact=lin_dact)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Train the network by backprop"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "i = 0\n",
      "lrate = 0.05\n",
      "while True:\n",
      "    nn0.forward(trn_x)\n",
      "    nn1.forward(nn0.outputs)\n",
      "    trn_err = np.mean(0.5*(nn1.outputs - trn_y)**2)\n",
      "    de_nn1_out = nn1.outputs - trn_y\n",
      "    nn1.backprop(de_nn1_out)\n",
      "    nn0.backprop(nn1.de_inputs)\n",
      "    \n",
      "    nn0.apply_grad(lrate)\n",
      "    nn1.apply_grad(lrate)\n",
      "    #nn0.forward(vld_x)\n",
      "    #nn1.forward(nn0.outputs)\n",
      "    #vld_err = np.mean(0.5*(nn1.outputs - vld_y)**2)\n",
      "    vld_err = 0\n",
      "    \n",
      "    print \"epoch %d: trn err %.3f vld err %.3f\" % (i, trn_err, vld_err)\n",
      "    \n",
      "    i = i+1\n",
      "    if i>200:\n",
      "        break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t2 = np.arange(0,np.pi,0.01)\n",
      "nn0.forward(t2[:,np.newaxis])\n",
      "nn1.forward(nn0.outputs)\n",
      "pred2 = nn1.outputs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as pl\n",
      "pl.plot(t2, pred2[:,0])\n",
      "pl.plot(trn_x[:,0], trn_y[:,0])\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "*TODO* Cross validation to selection a good model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sparse Coding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$X$ is a $k\\times m$ data matrix, where $m$ is the number of samples, $k$ being the number of features. Let the dictionary be $B$  of  $k\\times n$ and the coefficient matrix $S$ $of n\\times m$: $n$ coefficients per sample of the $m$ samples. The problem being:\n",
      "$$\n",
      "\\min_{B,S}\t\\sigma^{-2}\\|X-BS\\|_{F}^{2}+\\beta\\sum_{i,j}|s_{ij}| \\\\\n",
      "s.t.\t\\sum_{i}B_{ij}^{2}\\leq c,\\ j=1\\dots n\n",
      "$$\n",
      "  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fixing $B$, the objective w.r.t. $s$ is $$ \\xi^{T}\\xi-\\xi^{T}Bs-s^{T}B^{T}\\xi+s^{T}B^{T}Bs+\\lambda\\sum_{j}|s_{j}| $$. So the gradient to $s$  is $2B^{T}(Bs-\\xi)$  (a bit familiar of the backprop  computation of attributing error-derivatives back to the inputs? -- all summing over some features -- the inner-dimension of the matrix multiplication), and the penalty gradient, a vector of $\\{\\pm\\lambda,0\\}$, depending on the current value of $s$. The gradients can be computed as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for s, x in S, X:\n",
      "    ds0 = np.dot(B.T, np.dot(B,s)-x)\n",
      "    ds1 = lam * np.sign(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it is intuitive that ds0 looks like a perceptron error, and ds1 being the penalty of not being 0 -- note it is a constant penalty as long as you are not zero -- so the method gives you sparse solutions! \n",
      "\n",
      "But the problem is that if you push your s along ds0+ds1, ds1 could jump rigorously, and hamper you from convergence. Naturally, as an antidose to the jumping behaviour, we want to keep the momentum of the optimisation w.r.t. penalties. E.g. for a few steps, just keep the previous direction (the current choice of $\\pm \\lambda, 0$), despite a coefficient may cross 0 during the process.\n",
      "\n",
      "Why not simply using a larger step size? We want to gradient of the actual error [ds0] to guide our search. So the idea is not to play the trick on every one, but only on those coefficients that are strongly affecting the prediction performance (changing them greatly help reduce the errors). How strong? The error gradient should be greater than $\\lambda$. Then you can push them for a few steps assuming they are penalised in the same way as they are currently.\n",
      "\n",
      "Consider that the error dictates some $s_j$ to be smaller, i.e. the $\\partial E/\\partial s_j > 0$. If **(A)** $\\partial E$ is not strong enough, say $ \\lt +\\lambda$, then $s_j$ is dominated by the penalty. \n",
      "\n",
      "If the error gradient is strong, exceeding the penalty gradient. Then consider **(B)** if currently $s_j$ is already \"over the line\", $s_j \\lt 0$, the penalty $\\partial P/\\partial s_j = -\\lambda$ pushes $s_j$ to be larger (closer to 0), but will be overwhelmed by  the demand of error-reducing, so $s_j$ will become smaller. <del> Moreover, the solution must be some $s_j^* \\lt s_j$, otherwise, if you fix all other $s_{k\\neq j}$ and examine $s_j$, you would find convexity has been violated (by the \"otherwise\").</del> The above argument is not accurate according to our discussion. The condition I proposed may not be enough to preserve the sign of a coefficient.\n",
      "\n",
      "The complex condition is **(C)** when currently $s_j>0$, so now the penalty is \"helping\" us, it drives $s_j$ to where it would put error down. However, as long as we cross the zero-line, the penalty will turn against us. The hope is that initially our error-gradient is strong, so it might overcome the changed penalty, but this is no guarantee. So you check every time some coefficient changing its sign, pick and choose the best one. \n",
      "\n",
      "Anyway, the active set is made of coefficients contributing significantly to the (reduction of) error.\n",
      "```python\n",
      "    # choose an element to join the active set\n",
      "    ina = (th==0).nonzero()[0]        # inactive index\n",
      "    g = np.dot(B.T, np.dot(B,s) - x)  # gradient to error\n",
      "    ing = np.abs(g[ina])\n",
      "    a_i = np.argmax(ing)              # leading contributor\n",
      "    if ing[ai]<=lam:                  # none-tobe activated, over!\n",
      "        return s, fn_err(s), g, th\n",
      "    th[ina[a_i]] = -sign(g[ina[a_i]]) # predict sign as if descended\n",
      "```            "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The line search inspected\n",
      "Let s0 be the active coefficients (the current value of coefficients in the current active set). So the logic goes we will **(1)** solve a least square problem with the coefficents being \"dragged-to-zero-according-to-current-value\"; and **(2)** check the sign changes along the way. \n",
      "\n",
      "### solution exists?\n",
      "Note in **(1)** the solution must exist:\n",
      "$$\n",
      "L^s = 0.5*\\| x - B s\\|^2 + \\sum_j (\\pm) s_j\n",
      "$$\n",
      "The norm is a convex function w.r.t. $s$. With those signs fixed, the second term is linear (also convex) w.r.t. $s$ as well. So a global minimum is guaranteed -- you won't shoot to nowhere.\n",
      "\n",
      "So we solves the equalibrium of the gradients as\n",
      "$$\n",
      "B^T (Bs - x) + \\lambda \\vec{\\theta} = 0\n",
      "$$\n",
      "```python\n",
      "    a = th.nonzero()[0]          # active set                          \n",
      "    s1 = np.linalg.solve(np.dot(B[:,a].T, B[:,a]), \n",
      "            (np.dot(B[:,a].T, x) - lam*th[a])\n",
      "         )\n",
      "```\n",
      "\n",
      "But a question arises: *is the problem always well-defined?* Since $B$ is over-complete, is it possible the active set has more than enough coefficients to manipulate, which renders the problem to be underdetermined? If this is impossible, is there a proof of it?\n",
      "\n",
      "### are we making progress?\n",
      "For step **(2)**, let us imagine a movement from the current coefficients to the least square solution -- *despite that we can analytically get the solution and just \"jump\" to it*. Keep in mind along the path, we are minimising $L^s$, but the true objective should be \n",
      "$$\n",
      "L = \\| x - B s\\|^2 + \\sum_j | s_j |\n",
      "$$\n",
      "In the first part of the journey, right before the first sign change of any $s_j$. $L^s$ and $L$ are the same and we are minimising the true goal. After a sign has been changed, our surrogate $L^s$ starts deviating from $L$. Say $s_{k0}$ changes its sign before any one else, at some $t_0 \\in (0,1]$, where $t=0$ means the origional coefficient $s_0$ and $t=1$ means the coefficient minimising $L^s$. Then we can be assured that between $(0,t_0)$ we are reducing $L$. \n",
      "\n",
      "### where to chop the line for a check?\n",
      "After that, before another $s_{k1}$ changes its sign, we are minimising\n",
      "\\begin{align}\n",
      "L^s &= \\| x - B s\\|^2 + \\sum_j p_j s_j \\\\\n",
      "& = \\| x - B s\\|^2 - |s_{k0}| + \\sum_{j \\neq k_0} | s_j | \n",
      "& = L  - 2|s_{k0}|\n",
      "\\end{align}\n",
      "So the previous path becomes suboptimal. However, as long as $|s_{k0}|$ is small, hopfully, the path is still a _decending_ one for $L$. Along this section, say, from $t_0$ to $t_1$, where another $s_{k1}$ changes sign, our surrogate _steadily_ deviates from the true $L$. We have no guarantee the path remains descending for $L$, but can just say, the rate of our surrogate getting worse is constant, _until $s_{k1}$ joins trouble makers' club_!\n",
      "```python\n",
      "flipping_at = -s0/(s1-s0) # t0, t1, t ...\n",
      "```\n",
      "For each t in flipping between $(0,1]$, the coefficient is $(1-t)s_0+ts_1$.\n",
      "```python\n",
      "fn_n2=lambda x:numpy.sum(x**x)\n",
      "\n",
      "interm_costs = [(t,(1-t)*s0+t*s1) for t in flipping_at if t>0.0 and t<=1.0]\n",
      "best_t, best_L = max(interm_costs, \n",
      "    key=lambda item:\n",
      "        fn_n2(x-np.dot(B[:,a],item[1]))*0.5 \n",
      "        +np.sum(np.abs(item[1])))\n",
      "```\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The following code has bugs in it and not working, but it makes an illustration and please help improve it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def solve_coeff(x, B, lam):\n",
      "    s = np.zeros(B.shape[1])\n",
      "    th = np.zeros(B.shape[1], dtype=int) # active flags\n",
      "    fn_err = lambda s_: np.sum((x-np.dot(B,s_))**2)*0.5 \\\n",
      "                      + np.sum(np.abs(s_))*lam\n",
      "    fn_n2=lambda x:numpy.sum(x**x)\n",
      "    new_acti = True\n",
      "    it = 0\n",
      "    max_it = 1\n",
      "    while True:\n",
      "        if new_acti:\n",
      "            ina = (th==0).nonzero()[0]\n",
      "            g = np.dot(B.T, np.dot(B,s) - x)\n",
      "            ing = np.abs(g[ina])\n",
      "            a_i = np.argmax(ing)\n",
      "            if ing[a_i]<=lam:\n",
      "                return s, fn_err(s), g, th\n",
      "            th[ina[a_i]] = -np.sign(g[ina[a_i]])\n",
      "            new_acti = False\n",
      "        \n",
      "        a = th.nonzero()[0]\n",
      "        s0 = s[a]\n",
      "        s1 = np.linalg.solve(np.dot(B[:,a].T, B[:,a]), \n",
      "                (np.dot(B[:,a].T, x) - lam*th[a])\n",
      "             )\n",
      "        \n",
      "        print s0\n",
      "        print s1\n",
      "        \n",
      "        flipping_at = -s0/(s1-s0)\n",
      "        interm_costs = [(t,(1-t)*s0+t*s1) for t in flipping_at \n",
      "                                          if t>0.0 and t<=1.0]\n",
      "        if len(interm_costs) == 0:\n",
      "            print \"No flipping\"\n",
      "            s[a]=s1\n",
      "            new_acti=True\n",
      "            continue\n",
      "        \n",
      "        best_t, best_sa = max(interm_costs,\n",
      "                              key=lambda item:\n",
      "                                  fn_n2(x-np.dot(B[:,a],item[1]))*0.5 \n",
      "                                 +np.sum(np.abs(item[1]))\n",
      "                             )\n",
      "        s[a] = best_sa\n",
      "        th[a[np.isclose(s[a],0)]] = 0\n",
      "        a2 = th.nonzero()[0]\n",
      "        if len(a2)==0:\n",
      "            print \"No active left\"\n",
      "            break\n",
      "        else:\n",
      "            g = np.dot(B.T, np.dot(B, s)-x) + lam*np.sign(s)\n",
      "            if np.abs(g[a2]).sum() < 1e-9:\n",
      "                print \"active optimal, try to add new\"\n",
      "                new_acti = True\n",
      "        \n",
      "        it += 1\n",
      "        if it > max_it:\n",
      "            print \"max iter reached\"\n",
      "            break\n",
      "        print it, fn_err(s), s\n",
      "    return s, fn_err(s), g, th\n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "B = np.random.uniform(-1,1,(3, 10))\n",
      "x = np.random.uniform(-1,1,(3,))\n",
      "lam = 0.01\n",
      "s, e, g, th = solve_coeff(x,B,lam)\n",
      "print s, np.dot(B,s)-x\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}