{
 "metadata": {
  "name": "",
  "signature": "sha256:0f4982fc99713e1716427e5c19784be4c0ba8b6b8810c43ad875b9d894e6a21f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Explain away\" makes inference difficult in NN.\n",
      "complementary priors - one layer a time.\n",
      "\n",
      "top two layers: _associated memory_ (remember how Hopfields treats neural nets?)\n",
      "\n",
      "joint distribution\n",
      "\n",
      "low-D _free energy landscape_ of the top level assoc. memory\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wait, it talks conditional distribution of \n",
      "If you are modelling $X|H$, say, $X| H_1, H_2$. \n",
      "$H_1$ and $H_2$ may not be connected at all. However, it is _knowing $X=1$_ that links $H_1$ and $H_2$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that, the \"complementary\" prior is more like a conceptual tool. How, if you have some reasonable belief about the meaning of $H_1$ and $H_2$, then you should choose a prior belief, so that when you observe $X$, the posterior should factored to be $P(H_1, H_2|X) = Q_1(H_1) \\times Q_2(H_2)$.\n",
      "\n",
      "Sure you don't, at least in the example of the earthquake and truck accident, the idea of complementary prior seems to be not sensible. However, in general, when you are modelling soming using latent variables. That is because you are not actually sure about what is the hidden factors that give rise of the observations. In thise case, to let the priors as independent as possible may seem to be a good idea.\n",
      "\n",
      "* Any connection to ICA can be drawn here? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Log likelihood of the data:\n",
      "Given $H_1, \\dots, H_M$, the $i$-th data feature has an activation signal of $H_1W_{1,i}+H_2W_{2,i}+\\dots$.\n",
      "\n",
      "```python\n",
      "\"\"\"\n",
      "Each row of H is a sample: the logic goes alike from one sample to the other, so now we concentrate on the elements within a row.\n",
      "For each X-feature, we need to combine H-variables using W, to write a matrix product, this needs to be a column in W: W[:,i] is a column vector of how to combine H-variables for i-th X-feature.\n",
      "\"\"\"\n",
      "A = np.dot(H, W)\n",
      "```\n",
      "\n",
      "And\n",
      "$1/(1+exp^{-A})$\n",
      "\n",
      "\\begin{align}\n",
      "log P(X|H) = \\log \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Deep belief nets\n",
      "### Adjusting $w_{ij}$ Part I: the outmost layer\n",
      "#### Part I-a: a digress to credit attribution methodology\n",
      "Now let's compute how the change of a weight $w_{ij}$ affects the\n",
      "likelihood of observed data $\\boldsymbol{v}=[v_{1},\\dots,v_{p}]$.\n",
      "Note $w_{ij}$ dictates the contribution of activating $v_{i}$ by\n",
      "$h_{j}$. So we consider \n",
      "\\begin{align*}\n",
      " & \\frac{\\partial}{\\partial w_{ij}}\\log P(v_{i}|h_{j})\\\\\n",
      "= & \\frac{\\partial}{\\partial w_{ij}}\\begin{cases}\n",
      "\\log(s_{i}) & v_{i}=1\\\\\n",
      "\\log(1-s_{i}) & v_{i}=0\n",
      "\\end{cases}\\\\\n",
      "= & \\left[v_{i}\\frac{\\partial\\log s_{i}}{\\partial w_{ij}}+(1-v_{i})\\frac{\\partial\\log(1-s_{i})}{\\partial w_{ij}}\\right]\\\\\n",
      "= & \\left[v_{i}(1-s_{i})h_{j}+(1-v_{i})(-s_{i})h_{j}\\right]\n",
      "\\end{align*}\n",
      "where $h_{j}$ comes out of $da_{j}/dw_{ij}$ and $a_{j}=\\sum_{j'}w_{ij'}h_{j'}$.\n",
      "Before we put things in order in this expression, let's recap the\n",
      "backprop in NN: if some $w$ connects input $x$ and output $y$,\n",
      "and the activation of $y$ is $a$, the likelihood derivative at $a$\n",
      "will be $y(1-y)\\frac{\\partial\\log(y^{y^{*}}(1-y)^{(1-y*)})}{\\partial y}=y^{*}-y$.\n",
      "So we have the contribution of $w$ is $(y^*-y)h$. Following this\n",
      "pattern, we can write the equation above as \n",
      "\\begin{align*}\n",
      " & \\left[v_{i}(v_{i}-s_{i})+(1-v_{i})(v_{i}-s_{i})\\right]h_{j}\\\\\n",
      "= & (v_{i}-s_{i})h_{j}\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Part I-b $w_{ij}$ in the outmost layer\n",
      "The discussion above gives us a gradient of the similar form as given in (2). However, it won't lead us to (4). We have assumed some known ancestor $h$, _implicitly_. In a probabilistic treatment, this comes explicit. Let's go.\n",
      "\\begin{align*}\n",
      "\\frac{\\partial\\log P(v^{0})}{\\partial w_{ij}} & =\\frac{1}{P(v^{0})}\\frac{\\partial P(v^{0})}{\\partial w_{ij}}\n",
      "\\end{align*}\n",
      "So we are concerned with how $w_{ij}$ affects the definition of $P(v^{0})$--it\n",
      "is through $h^{0}$, so (let's omit the layer superscript for now):\n",
      "\\begin{align*}\n",
      "\\frac{\\partial P(v)}{\\partial w_{ij}} & =\\frac{\\partial \\sum_h P(v|h)P(h)}{\\partial w_{ij}}\n",
      "\\end{align*}\n",
      "We have a closer look into the probability $P(v|h)$. Through weights,\n",
      "$h$ activates individual elements in $v$ -- meaning that given $h$,\n",
      "the observed features are explained one by one -- this the Bayesian\n",
      "network, the arrows are from $h$ to $v$, so $v|h$ is the easy part\n",
      "(consider the \"explain away\") effect in the $h$ side given $v$\n",
      "as we have discussed above. \n",
      "\n",
      "> But on the other side, __how does this model setting suits the data__?\n",
      "Should we expect $h$ captures all the structural information? For\n",
      "example, if $v$ represents an image, and $h$ encodes that image,\n",
      "the belief net $h\\rightarrow v$ means: if one knows $h$, the code\n",
      "of the image, then he has all the meaningful structure of the image,\n",
      "if he is further given half of the pixels in $v$, that is of little\n",
      "use to him, because _all $v_{i}$ can tell about $v_{j}$ has already\n",
      "been encoded in $h$_. This seems to be unpractical, of course, check\n",
      "the work of **gated MRF**. \n",
      "\n",
      "> If, on the other hand, on sticks to the idea that _$h$ should encode the data so much so that one observed feature should not help the inference of the other_. This can be stated alternatively as _$h$ should encode the data so much so that one unobserved feature should not hurt the inference of the other_ -- so check the work of **denoising auto encoders**.\n",
      "\n",
      "Get's back, $w_{ij}$ is affecting an objective function like\n",
      "\\begin{align*}\n",
      "\\frac{\\partial P(v)}{\\partial w_{ij}} & =\\frac{\\partial\\sum_{h}P(v_{1}|h)\\times P(v_{2}|h)\\times\\dots\\times P(v_{p}|h)\\times P(h)}{\\partial w_{ij}}\\\\\n",
      " & =\\sum_{h}P(h)\\sum_{k}\\left(\\frac{\\partial P(v_{k}|h)}{\\partial w_{ij}}\\prod_{k'\\neq k}P(v_{k'}|h)\\right)\n",
      "\\end{align*}\n",
      "Only when $k=i$, $w_{ij}$ will affect $P(v_{k}|h)$ and $\\frac{\\partial P(v_{k}|h)}{\\partial w_{ij}}\\neq0$,\n",
      "so \n",
      "\\begin{align*}\n",
      "= & P(h)\\frac{\\partial P(v_{i}|h)}{\\partial w_{ij}}\\prod_{i'\\neq i}P(v_{i'}|h)\n",
      "\\end{align*}\n",
      "Let's look into $P(v_{i})$, let $\\mathbf{1}[\\cdot]$ be the logical\n",
      "true, $s_{i}=(1+e^{-a_{i}})$, $a_{i}$ being the total input to $v_{i}$\n",
      "(so $da_{i}/dw_{ij}=h_{j}$) \n",
      "\\begin{align*}\n",
      "\\frac{\\partial P(v_{i}|h)}{\\partial w_{ij}} & =\\frac{\\partial}{\\partial w_{ij}}\\left(\\mathbf{1}[v_{i}=1]P(v_{i}=1|h)+\\mathbf{1}[v_{i}=0]\\left(1-P(v_{i}=1|h)\\right)\\right)\\\\\n",
      " & =\\frac{\\partial}{\\partial w_{ij}}(v_{i}s_{i}+(1-v_{i})(1-s_{i}))\\\\\n",
      " & =v_{i}s_{i}(1-s_{i})h_{j}+(1-v_{i})(-s_{i})(1-s_{i})h_{j}\\\\\n",
      " & =v_{i}s_{i}(v_{i}-s_{i})h_{j}+(1-v_{i})(v_{i}-s_{i})(1-s_{i})h_{j}\\\\\n",
      " & =(v_{i}s_{i}+(1-v_{i})(1-s_{i}))(v_{i}-s_{i})h_{j}\\\\\n",
      " & =P(v_{i}|h)(v_{i}-s_{i})h_{j}\n",
      "\\end{align*}\n",
      "Put it into the expression above, we have\n",
      "\\begin{align*}\n",
      "\\frac{\\partial\\log P(v)}{\\partial w_{ij}} & =\\frac{1}{P(v)}\\sum_{h}P(h)P(v|h)(v_{i}-s_{i})h_{j}\\\\\n",
      " & =\\sum_{h}P(h|v)(v_{i}-s_{i})h_{j}\\\\\n",
      " & =\\mathbb{E}_{h|v}\\left[(v_{i}-s_{i}(h))h_{j}\\right]\n",
      "\\end{align*}\n",
      "Note we write $s_{i}(h)$ to make the dependency explicit.\n",
      "\n",
      "> Interestingly, it does not matter very much whether you write $P(v_{i}|h)$\n",
      "as $\\mathbf{1}[v_{i}=1]P(v_{i}=1|h)+\\mathbf{1}[v_{i}=0]\\left(1-P(v_{i}=1|h)\\right)$\n",
      "or as $P(v_{i}=1|h)^{\\mathbf{1}[v_{i}=1]}\\times\\left(1-P(v_{i}=1|h)\\right)^{\\mathbf{1}[v_{i}=0]}$\n",
      "-- this may reflect the internal consistency of the definition of\n",
      "probability as discussed by ET Jaynes.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Adjusting $w$: Part II: one layer above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Above we have taken $P(h)$ as a piece of given information. But if\n",
      "we consider $P(h)$ as a population defined through layers above,\n",
      "it turns out to be depending on $w_{ij}$ as well. Let's re-check\n",
      "the influence of $w_{ij}$:\n",
      "\\begin{align*}\n",
      "\\frac{\\partial\\log P(v)}{\\partial w_{ij}} & =\\frac{1}{P(v)}\\frac{\\partial\\sum_{h}P(v|h)P(h)}{\\partial w_{ij}}\\\\\n",
      " & =\\frac{1}{P(v)}\\sum_{h}P(h)\\frac{\\partial P(v|h)}{\\partial w_{ij}}+\\frac{1}{P(v)}\\sum_{h}P(v|h)\\frac{\\partial P(h)}{\\partial w_{ij}}\n",
      "\\end{align*}\n",
      "The first term is what we have arrived above, now look at the second\n",
      "term, following the same line of derivation, to inspect how $w_{ij}$\n",
      "affects $P(h)$, we need to decompose $P(h)$ in terms of its ancestor.\n",
      "Now let's bring back our superscripts to make clear the layers:\n",
      "\\begin{align*}\n",
      "\\frac{\\partial P(h^{0})}{\\partial w_{ij}} & =\\frac{\\partial\\sum_{v^{1}}P(h^{0}|v^{1})P(v^{1})}{\\partial w_{ij}}\\\\\n",
      " & =\\sum_{v^{1}}P(v^{1})\\frac{\\partial P(h^{0}|v^{1})}{\\partial w_{ij}}\\\\\n",
      " & =\\sum_{v^{1}}P(v^{1})P(h^{0}|v^{1})(h_{j}^{0}-t_{j}^{0})v_{i}^{1}\n",
      "\\end{align*}\n",
      "$t_{j}^{0}$ is $P(h_{j}^{0}=1|v^{1})$ according to the model. So\n",
      "the logic goes: \n",
      "\\begin{align*}\n",
      " & \\frac{\\partial P(\\mathrm{Layer}^{\\mathrm{Down}}|\\mathrm{Layer}^{\\mathrm{Up}})}{\\partial\\mathrm{Connection}(\\textrm{Up-node},\\textrm{Down-node})}\\\\\n",
      "= & P(\\mathrm{Layer}^{\\mathrm{Down}}|\\mathrm{Layer}^{\\mathrm{Up}})\\\\\n",
      " & \\times\\textrm{Up-node-value}\\\\\n",
      " & \\times(\\textrm{Down-node-value}-\\textrm{Down-node-mean})\n",
      "\\end{align*}\n",
      "Put together, "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The tale of two models\n",
      "#### Recap: RBM == DBN\n",
      "\n",
      "So now we have an RBM of data, and we have established the RBM is\n",
      "equivalent to an infinite NN\n",
      "\\begin{align*}\n",
      "P(X_{i}=1) & =(1+e^{-\\sum_{j}w_{ij}H_{j}})^{-1}\\\\\n",
      "P(H_{j}=1) & =(1+e^{-\\sum_{i}w_{ij}X_{i}})^{-1}\n",
      "\\end{align*}\n",
      "Now $w_{\\cdot,\\cdot}$ has been determined, and our RBM is a good\n",
      "model of the observed data. Now add another layer of belief to $H$\n",
      "to improve the representative power (why?). Say, $H$ is given a prior\n",
      "$p(H)$ via another RBM, and our previous RBM becomes an $H\\rightarrow X$\n",
      "net. Still, we have $P(X_{i}=1)$ defined exactly as above. The goal\n",
      "is that using the new model to define a $P(H)$, making $\\sum_{H}P(X|H)P(H)$\n",
      "a better model than the previous RBM $\\sum_{H}P_{\\textrm{RBM-0}}(X,H)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### We need an example here showing the equivlance\n",
      "> Improvise some RBM and a DBN\n",
      "\n",
      "> Using the RBM and the DBN to generate 1e+6 samples, for DBN, say, we do 100 layers\n",
      "\n",
      "> Compre the sample mean"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### A digress to EM and model approximation\n",
      "\n",
      "\n",
      "Maybe the first thing to do is being able to compare $\\log P(x)$\n",
      "under the two models (note lower $x$ means actual observations of\n",
      "$X$, say, $P(X_{1}=x_{1},X_{2}=x_{2},\\dots)$). Write it as \n",
      "\\begin{align*}\n",
      "\\log P(x) & =\\sum_{h}Q(h)\\log P(x)\n",
      "\\end{align*}\n",
      "Now $Q(h)$ is just ANY weight (measure) that enables you to count\n",
      "on every $h$ about something (here $\\log P(x)$). We have not establish\n",
      "any connection between $X$ and $H$: the basis $h$-values has little\n",
      "to do with the thing of interest ($\\log P(x)$). You are just stating\n",
      "$\\log P(x)=0.3\\log P(X)+0.1\\log P(x)+0.6\\log P(x)$. So, why borthering?\n",
      "You can make sense from items cancelling each other out - the key\n",
      "is to inject the actual relation $P(x,h)$ and see how it interplays\n",
      "with your arbitrary $Q$ \n",
      "\\begin{align*}\n",
      "= & \\sum_{h}Q(h)\\log P(x)\\frac{Q(h)}{Q(h)}\\frac{P(x,h)}{P(x,h)}\\\\\n",
      "= & \\sum_{h}Q(h)[-\\log Q(h)+\\log P(x,h)+\\log\\frac{Q(h)}{\\frac{P(x,h)}{P(x)}}]\\\\\n",
      "= & \\mathbb{H}[Q]+\\mathbb{E}_{Q}[\\log P(x,h)]+\\mathbf{KL}[Q\\|P_{H|X}]\n",
      "\\end{align*}\n",
      "It turns out, often you can just throw out any $Q$, and say ``Look,\n",
      "you ain't going anywhere if tring to add all those $h$'s. So you'll\n",
      "buy this guy ($Q$) as $P(H|X)$ and see what we can do to improve\n",
      "$P(X)$'' Then it turns out you only need to improve $\\mathbb{E}_{Q}[\\log P(x,h)]$,\n",
      "because if $Q$ is $P_{H|X}$, the $\\mathbf{KL}[]$ term vanishes.\n",
      "\\textbf{Note} by adjusting the model you are obviously trying to have\n",
      "your cake and eat it -- violating the assumption that $Q$ IS $P(H|X)$,\n",
      "nonetheless, the $Q$ was thrown out arbitrarily in the first place,\n",
      "so $\\mathbf{KL}[]$ does not vanish anyway and we are improving a\n",
      "lower bound of $\\log P(x)$, not accouting for $\\mathbf{KL}[]$. $\\mathbb{E}_{Q}[\\log P(x,h)]$\n",
      "is often easy to handle, or at least you can make it so by picking\n",
      "a convenient $Q$. Then the estimating step is to choose a better\n",
      "$Q$, the cycle goes: improve lower bound, tighten it (by bringing\n",
      "up the actual $\\log P(X)$, in turn by squeezing $\\mathbf{KL}[]$),\n",
      "improve lower bound again, ... (Is it to be a bit persistent, e.g.\n",
      "when squeezing $\\mathbf{KL}[]$, try to forsee how it would affect\n",
      "further $\\mathbb{E}_{Q^{New}}\\log P(x,h)$?)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Model series\n",
      "\n",
      "\n",
      "\n",
      "Back to our interested problem of comparing the two models. First\n",
      "let's be explicit about the detailed dependency on model parameters\n",
      "of $\\log P^{\\textrm{DBN}}$. When all weights are tied, and we have\n",
      "DBN is the same as RBM, let's say at this moment, the two models _match_,\n",
      "when we release a layer of DBN, so the $W$ connecting $H^{0}$ and\n",
      "$X^{0}$ is different from those connecting $H^{1}$ and layers upwords,\n",
      "let's say at this moment the models _detach_. We first\n",
      "write down the terms of $\\log P(x)$, which is the goal, and how those\n",
      "terms change in the detach step:\n",
      "\\begin{align*}\n",
      "\\log P(X) & =\\mathbb{H}[Q]+\\mathbb{E}_{Q}[\\log P(x,h)]+\\mathbf{KL}[Q\\|P_{H|X}]\\\\\n",
      " & =\\mathbb{H}[Q]+\\mathbb{E}_{Q}[\\log P(x|h)]+\\mathbb{E}_{Q}[\\log P(h)]+\\mathbf{KL}[Q\\|P_{H|X}]\n",
      "\\end{align*}\n",
      "\n",
      "<img src=\"files/tab1.tiff\">\n",
      "\n",
      "\n",
      "\n",
      "The detach step will try to take those $H$'s (in practice probably\n",
      "only one) sampled from $Q$ as ``observations'', and improve the\n",
      "likelihood in a new RBM (Equivalently all upper layers of the new\n",
      "DBN). So we hope $\\mathbb{E}_{Q}[\\log P^{\\textrm{DBN-New}}(H)]$ is\n",
      "better than $\\mathbb{E}_{Q}[\\log P^{\\textrm{DBN-Old}}(H)]$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Understanding how it works__\n",
      "\n",
      ">Wait a moment, the old DBN use $Q$ to approximate the population\n",
      "of $H$, then from $Q$ you draw $H$-samples, and then you want to\n",
      "improve the likelihood of observing those samples in the new DBN model.\n",
      "Shouldn't we ask how you can possibly achieve an improvement? The\n",
      "$H$-samples are drawn from the old model after all! The subtle issue\n",
      "is that the old model use $Q$ to approximate _posterior_ of\n",
      "$H$. But now we are to improve the _prior_ of $H$. It makes\n",
      "sense because the ultimate goal is to enhance the probability of generating\n",
      "the observed $X$ from the model. Before detaching, we have optimised\n",
      "(or partially) $P_{X}$ in the first RBM/DBN model, _and_ have\n",
      "the population of $H|X$. Now we leave $X|H$ untouched (see table\n",
      "above) -- so the model will be in a good position to generate the\n",
      "observed $X$ if it is tuned to be favouring those $H$ in the interplay\n",
      "with $X$ in the first RBM, i.e. $H|X$ or $Q(H)$. Without observing\n",
      "$X$ in the first place, the first DBN/RBM will randomly put their\n",
      "$H$, so our model will be better shaped if we can let $H$, now called\n",
      "$H^{(0)}$, be distributed as the posterior _a priori_. So at\n",
      "detach step, $P_{H}$ improves, hopefully.\n",
      "\n",
      "> Take $\\mathbf{KL}[Q\\|P_{H|X}]$ as a bonus: our approximate of $\\log P(X)$\n",
      "goes up, _and_ becomes inaccurate, so the actual $\\log P(X)$\n",
      "will be upper!\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# An example of how this step improves the data log-likelihood\n",
      "# It won't be easy to compute the true log-likelihood and say look, I am improving!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}